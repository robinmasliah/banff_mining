{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tika\n",
    "from tika import parser\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import itertools\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from random import randint\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "#from gensim.models import KeyedVectors\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, classification_report, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get data file names:\n",
    "path = r'/Users/robinmasliah/Documents/BanffMining/biopsy_scrap/'\n",
    "filenames = glob.glob(path + \"/*.pdf\")\n",
    "\n",
    "pdfs = []\n",
    "for filename in filenames:\n",
    "    pdfs.append(parser.from_file(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# À compléter\n",
    "# fibrinoid necrosis => v3\n",
    "# transmural inflammation => v3\n",
    "\n",
    "medical_words = {\n",
    "    \"nb_glomeruli\" : ['number of glomeruli', 'glomeruli'],\n",
    "    \"glom_scler\" : ['globally sclerotic'],\n",
    "    \"g\": ['glomerulitis', 'marginated cells', 'acute allograft glomerulopathy'],\n",
    "    \"i-ifta\": ['inflammatory cell infiltrate'],\n",
    "    \"i\": ['tubulointerstitial', 'infiltrate', 'inflammatory', 'unscarred cortical inflammation', 'inflammatory cells'],\n",
    "    \"t\": ['tubulitis'],\n",
    "    \"ti\": ['total inflammation', 'total cortical inflammation scarred and unscarred'],\n",
    "    \"ci\": ['interstitial fibrosis', 'tubulointerstitial fibrosis', 'interstitium'],\n",
    "    \"ct\": ['tubular atrophy', 'atrophic tubules', 'tubules'],\n",
    "    \"cv\": ['fibrous intimal thickening', 'fibrous intimal arteriosclerosis', \n",
    "           'intimal fibrosis', 'arterial sclerosis', 'atherosclerosis', 'thickened intima',\n",
    "          'vessels', 'arteries', 'arterioles'],\n",
    "    \"v\": ['endothelialitis', 'trombosis', 'intimal arteritis', 'endarteritis', 'endovasculitis',\n",
    "         'endotheliitis', 'endarteritis', 'fibrinoid necrosis', 'transmural inflammation',\n",
    "         'vessels', 'arteries', 'arterioles'],\n",
    "    \"ah\": ['arteriolar hyalinosis', 'vessels', 'arteries', 'arterioles', 'arteriolar hyaline'],\n",
    "    \"mm\": ['mesangial matrix increase '],\n",
    "    \"ptc\": ['peritubular capilaritis', 'peripheral capilaries','peritubular capillary', \n",
    "            'peritubular capillaries', 'peritubular capillaritis'],\n",
    "    \"cg\": ['transplant glomerulopathy', 'glomerular basement membranes', 'doubles countours', \n",
    "           'double contour'],\n",
    "    'c4d' : ['c4d']\n",
    "    \n",
    "}\n",
    "\n",
    "degrees = {\n",
    "    \"-1\": [' negativity ', ' negative '],\n",
    "    \"+1\": [' positivity ', ' positive '],\n",
    "    \"0\": [' no ', ' not ', ' non-', 'minimal', '5%', 'without', 'none'],\n",
    "    \"1\": ['mild', ' is noted', 'focal', '15-20%'],\n",
    "    \"2\": ['moderate', '30%'],\n",
    "    \"3\": ['severe', '50%', '95%', 'dense']\n",
    "}\n",
    "\n",
    "numbers = {\n",
    "    \"0\": [' zero '],\n",
    "    \"1\": [' one '],\n",
    "    \"2\": [' two '],\n",
    "    \"3\": [' three '],\n",
    "    \"4\": [' four '],\n",
    "    \"5\": [' five '],\n",
    "    \"6\": [' six '],\n",
    "    \"7\": [' seven '],\n",
    "    \"8\": [' eight '],\n",
    "    \"9\": [' nine '],\n",
    "    \"10\": [' ten '],\n",
    "}\n",
    "\n",
    "banff_list = ['nb_glomeruli', 'glom_scler', ' g ', ' i ', ' t ', \n",
    "              ' ti ', ' ci ', ' ct ', ' cv ', ' v ', ' ah ',\n",
    "             ' mm ', ' ptc ', ' cg ', ' c4d ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Levenstein distance\n",
    "def LD(s, t):\n",
    "    \n",
    "    if s == \"\":\n",
    "        return len(t)\n",
    "    if t == \"\":\n",
    "        return len(s)\n",
    "    if s[-1] == t[-1]:\n",
    "        cost = 0\n",
    "    else:\n",
    "        cost = 1\n",
    "\n",
    "    res = min([LD(s[:-1], t) + 1,\n",
    "               LD(s, t[:-1]) + 1,\n",
    "               LD(s[:-1], t[:-1]) + cost])\n",
    "    return res\n",
    "\n",
    "# Text cleaning\n",
    "def replace(m_string):\n",
    "    return m_string.replace(\":\", \"\").replace(\";\", \"\").replace(\")\", \"\").replace(\"(\", \"\").replace(\"\\u200b\", \"\").lower()\n",
    "\n",
    "# Remove stopwords\n",
    "def remove_stopwords(word_list):\n",
    "    '''''\n",
    "    I : Liste de mots\n",
    "    O : Liste de mots filtrée\n",
    "    '''''\n",
    "    return [word for word in word_list if word not in stopwords.words('english')]\n",
    "\n",
    "# regex\n",
    "def regex_transformation(regex, str):\n",
    "    \n",
    "    t = regex.findall(str)\n",
    "    t = ''.join(t)\n",
    "    match_number = re.compile('-?\\ *[0-9]+\\.?[0-9]*(?:[Ee]\\ *-?\\ *[0-9]+)?')\n",
    "    t = [float(x) for x in re.findall(match_number, t)]\n",
    "    return t\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "alphabets = \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "\n",
    "# split texte en listes de phrases\n",
    "def split_into_sentences(text):\n",
    "    \n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = re.sub(prefixes, \"\\\\1<prd>\", text)\n",
    "    text = re.sub(websites, \"<prd>\\\\1\", text)\n",
    "    if \"Ph.D\" in text:\n",
    "        text = text.replace(\"Ph.D.\", \"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \", \" \\\\1<prd> \", text)\n",
    "    text = re.sub(acronyms + \" \" + starters, \"\\\\1<stop> \\\\2\", text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" +\n",
    "                  alphabets + \"[.]\", \"\\\\1<prd>\\\\2<prd>\\\\3<prd>\", text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets +\n",
    "                  \"[.]\", \"\\\\1<prd>\\\\2<prd>\", text)\n",
    "    text = re.sub(\" \" + suffixes + \"[.] \" + starters, \" \\\\1<stop> \\\\2\", text)\n",
    "    text = re.sub(\" \" + suffixes + \"[.]\", \" \\\\1<prd>\", text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\", \" \\\\1<prd>\", text)\n",
    "    if \"”\" in text:\n",
    "        text = text.replace(\".”\", \"”.\")\n",
    "    if \"\\\"\" in text:\n",
    "        text = text.replace(\".\\\"\", \"\\\".\")\n",
    "    if \"!\" in text:\n",
    "        text = text.replace(\"!\\\"\", \"\\\"!\")\n",
    "    if \"?\" in text:\n",
    "        text = text.replace(\"?\\\"\", \"\\\"?\")\n",
    "    text = text.replace(\".\", \".<stop>\")\n",
    "    text = text.replace(\"?\", \"?<stop>\")\n",
    "    text = text.replace(\"!\", \"!<stop>\")\n",
    "    text = text.replace(\"<prd>\", \".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences\n",
    "\n",
    "# Bug des listes dans le dataframe\n",
    "def clean_table(appended_data):\n",
    "    \n",
    "    for i, var in enumerate(appended_data):\n",
    "        appended_data[var] = appended_data[var].str[0]\n",
    "    return appended_data\n",
    "\n",
    "# Prend les valeurs dans le texte\n",
    "def search_direct_values(str):\n",
    "\n",
    "    str = replace(str)\n",
    "\n",
    "    regex_glomeruli = re.compile('number of glomeruli -?\\ *[0-9]+\\.?[0-9]*(?:[Ee]\\ *-?\\ *[0-9]+)?')\n",
    "    regex_glom_scler = re.compile('number globally sclerotic -?\\ *[0-9]+\\.?[0-9]*(?:[Ee]\\ *-?\\ *[0-9]+)?')\n",
    "    regex_g = re.compile(' g -?\\ *[0-9]+\\.?[0-9]*(?:[Ee]\\ *-?\\ *[0-9]+)?')\n",
    "    regex_i = re.compile(' i -?\\ *[0-9]+\\.?[0-9]*(?:[Ee]\\ *-?\\ *[0-9]+)?')\n",
    "    regex_v = re.compile(' v -?\\ *[0-9]+\\.?[0-9]*(?:[Ee]\\ *-?\\ *[0-9]+)?')\n",
    "    regex_t = re.compile(' t -?\\ *[0-9]+\\.?[0-9]*(?:[Ee]\\ *-?\\ *[0-9]+)?')\n",
    "    regex_ah = re.compile(' ah -?\\ *[0-9]+\\.?[0-9]*(?:[Ee]\\ *-?\\ *[0-9]+)?')\n",
    "    regex_cg = re.compile(' cg -?\\ *[0-9]+\\.?[0-9]*(?:[Ee]\\ *-?\\ *[0-9]+)?')\n",
    "    regex_mm = re.compile(' mm -?\\ *[0-9]+\\.?[0-9]*(?:[Ee]\\ *-?\\ *[0-9]+)?')\n",
    "    regex_ci = re.compile(' ci -?\\ *[0-9]+\\.?[0-9]*(?:[Ee]\\ *-?\\ *[0-9]+)?')\n",
    "    regex_ct = re.compile(' ct -?\\ *[0-9]+\\.?[0-9]*(?:[Ee]\\ *-?\\ *[0-9]+)?')\n",
    "    regex_cv = re.compile(' cv -?\\ *[0-9]+\\.?[0-9]*(?:[Ee]\\ *-?\\ *[0-9]+)?')\n",
    "    regex_ptc = re.compile(' ptc -?\\ *[0-9]+\\.?[0-9]*(?:[Ee]\\ *-?\\ *[0-9]+)?')\n",
    "    regex_ti = re.compile(' ti -?\\ *[0-9]+\\.?[0-9]*(?:[Ee]\\ *-?\\ *[0-9]+)?')\n",
    "\n",
    "\n",
    "    nb_glomeruli = regex_transformation(regex_glomeruli, str)\n",
    "    glom_scler = regex_transformation(regex_glom_scler, str)\n",
    "    g = regex_transformation(regex_g, str)\n",
    "    i = regex_transformation(regex_i, str)\n",
    "    v = regex_transformation(regex_v, str)\n",
    "    t = regex_transformation(regex_t, str)\n",
    "    ah = regex_transformation(regex_ah, str)\n",
    "    cg = regex_transformation(regex_cg, str)\n",
    "    mm = regex_transformation(regex_mm, str)\n",
    "    ci = regex_transformation(regex_ci, str)\n",
    "    ct = regex_transformation(regex_ct, str)\n",
    "    cv = regex_transformation(regex_cv, str)\n",
    "    ptc = regex_transformation(regex_ptc, str)\n",
    "    ti = regex_transformation(regex_ti, str)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(columns=['glomerulis', 'g', 'i', 't', 'v',\n",
    "                               'ah', 'cg', 'ci', 'ct', 'ti', 'cv', \n",
    "                               'mm', 'ptc', 'glom_scler'])\n",
    "\n",
    "    y = {'glomerulis': nb_glomeruli, 'g': g, 'i': i, \n",
    "         't': t, 'v': v, 'ah': ah, 'cg': cg, 'ci': ci, \n",
    "         'ct': ct, 'ti': ti, 'cv': cv, 'mm': mm, \n",
    "         'ptc': ptc, 'glom_scler': glom_scler\n",
    "        }\n",
    "\n",
    "    df.loc['y'] = y\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_ifta(df):\n",
    "    \n",
    "    if(df.ci.values[0] == 0 and df.ct.values[0] == 1) or (df.ci.values[0] == 1 and df.ct.values[0] == 0) or (df.ci.values[0] == 1 and df.ct.values[0] == 1):\n",
    "        df['IFTA'] = 1\n",
    "    elif(df.ci.values[0] == 2 and df.ct.values[0] == 2) or (df.ci.values[0] == 2 or df.ct.values[0] == 2):\n",
    "         df['IFTA'] = 2\n",
    "    elif(df.ci.values[0] == 3 and df.ct.values[0] == 3) or (df.ci.values[0] == 3 or df.ct.values[0] == 3):\n",
    "         df['IFTA'] = 3\n",
    "    else:\n",
    "        df['IFTA'] = 0\n",
    "    return df\n",
    "\n",
    "def get_banff_code(dictOfElements, valueToFind):\n",
    "    for code, name in medical_words.items():\n",
    "        if valueToFind in name:\n",
    "            return ' ' + code + ' '\n",
    "\n",
    "def get_degree(dictOfElements, valueToFind):\n",
    "    for code, name in degrees.items():\n",
    "        if valueToFind in name:\n",
    "            return ' ' + code + ' '\n",
    "        \n",
    "def get_numbers(dictOfElements, valueToFind):\n",
    "    for code, name in numbers.items():\n",
    "        if valueToFind in name:\n",
    "            return ' ' + code + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "print(get_banff_code(medical_words, 'glomeruli'))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appended_data = []\n",
    "for i, pdf in enumerate(pdfs):\n",
    "    df = search_direct_values(pdfs[i]['content'])\n",
    "    appended_data.append(df)\n",
    "appended_data = pd.concat(appended_data)\n",
    "appended_data = appended_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appended_data = clean_table(appended_data)\n",
    "d = add_ifta(appended_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saved into database CSV\n",
    "file_name = 'banff_table.csv'\n",
    "appended_data.to_csv(file_name, sep=',', encoding='utf-8')\n",
    "appended_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "split_into_sentences(pdfs[0]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data file names:\n",
    "path = r'/Users/robinmasliah/Documents/BanffMining/files_to_test'\n",
    "filenames = glob.glob(path + \"/*.pdf\")\n",
    "\n",
    "pdfs = []\n",
    "for filename in filenames:\n",
    "    pdfs.append(parser.from_file(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, file in enumerate(filenames):\n",
    "    print(idx, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medical_flat_list = [item for sublist in medical_words.values() for item in sublist]\n",
    "degrees_flat_list = [item for sublist in degrees.values() for item in sublist]\n",
    "numbers_flat_list = [item for sublist in numbers.values() for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_pdf = []\n",
    "for idx, pdf in enumerate(pdfs):\n",
    "    liste_pdf.append(pdfs[idx]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text detection\n",
    "\n",
    "start = 'LIGHT MICROSCOPY'\n",
    "end = 'IMMUNOFLUORESCEN'\n",
    "\n",
    "def loop_text(start, end, liste_pdf):\n",
    "    list_text = []\n",
    "    for idx, text in enumerate(liste_pdf):\n",
    "        text = (text.split(start))[1].split(end)[0].lower()\n",
    "        text = text.lower().strip().replace('\\n', ' ').replace('  ', ' ').replace('[^\\w\\s]','')\n",
    "        for i in medical_flat_list:\n",
    "            if i in text:\n",
    "                text = text.replace(i, get_banff_code(medical_words, i))\n",
    "        \n",
    "        for i in numbers_flat_list:\n",
    "            if i in text:\n",
    "                text = text.replace(i, get_numbers(numbers, i))\n",
    "\n",
    "        for i in degrees_flat_list:\n",
    "            if i in text:\n",
    "                text = text.replace(i, get_degree(degrees, i))\n",
    "        list_text.append(split_into_sentences(text))\n",
    "    return list_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_text = loop_text(start, end, liste_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recherche par mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation du dataset\n",
    "var0 = 0\n",
    "var1 = 0\n",
    "var2 = 0\n",
    "var3 = 0\n",
    "var4 = 0\n",
    "var5 = 0\n",
    "var6 = 0\n",
    "var7 = 0\n",
    "var8 = 0\n",
    "var9 = 0\n",
    "var10 = 0\n",
    "var11 = 0\n",
    "var12 = 0\n",
    "var13 = 0\n",
    "\n",
    "df = pd.DataFrame(columns=['glomerulis', 'g', 'i', 't', 'v', \n",
    "                           'ah', 'cg', 'ci', 'ct', 'ti', 'cv', \n",
    "                           'mm', 'ptc', 'glom_scler'])\n",
    "\n",
    "y = {'glomerulis': var0, 'g': var2, 'i': var3, \n",
    "     't': var4, 'v': var5, 'ah': var6, 'cg': var7, \n",
    "     'ci': var8, 'ct': var9, 'ti': var10, 'cv': var11, \n",
    "     'mm': var12, 'ptc': var13, 'glom_scler': var1\n",
    "    }\n",
    "\n",
    "df.loc['y'] = y\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mots à enlever = ['do', 'or', 'on', 'is', ':']\n",
    "\n",
    "def rand_int_text():\n",
    "    random_text = randint(0, 20)\n",
    "    return random_text\n",
    "\n",
    "print(rand_int_text())\n",
    "sentences = list_text[rand_int_text()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find glomeruli and glom_scler\n",
    "\n",
    "for i in sentences:\n",
    "    if(i.find('glomeruli') != -1):\n",
    "        phrase = i\n",
    "        break\n",
    "phrase\n",
    "nb_glomeruli = (re.findall(r'\\d+', phrase))\n",
    "print(nb_glomeruli)\n",
    "\n",
    "nb_glomeruli_1 = nb_glomeruli[0]\n",
    "glom_scler = nb_glomeruli[1]\n",
    "\n",
    "try:\n",
    "    print('nb_glomeruli : ', nb_glomeruli_1)\n",
    "    print('glom scler : ', glom_scler)\n",
    "    nb_glomeruli_1 = nb_glomeruli[0]\n",
    "    glom_scler = nb_glomeruli[1]\n",
    "except Exception as e:\n",
    "    print(isinstance(e, NameError))\n",
    "\n",
    "var0 = nb_glomeruli_1\n",
    "var1 = glom_scler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find g\n",
    "new_list = []\n",
    "for i in sentences:\n",
    "    if((\" g \" in i) == True):\n",
    "        i = ' '.join([word for word in i.split() if word not in (stopwords.words('english'))])\n",
    "        i = i.split()\n",
    "        print(i)\n",
    "        break\n",
    "    else:\n",
    "        g = 0\n",
    "    \n",
    "try:        \n",
    "    for word in i:\n",
    "        if(len(word) <= 2):\n",
    "            new_list.append(word)\n",
    "    print(new_list)\n",
    "\n",
    "\n",
    "    phrase = ' '.join(new_list)\n",
    "    print(phrase)\n",
    "    g = re.findall(r'g (\\d+)', phrase)\n",
    "    if((g == []) is True):\n",
    "        g = re.findall(r'(\\d+) g', phrase)\n",
    "    \n",
    "    print(g[0])\n",
    "    var2 = g[0]\n",
    "    print('g = ', var2)\n",
    "\n",
    "except IndexError as e:\n",
    "    var2 = 0\n",
    "    print(var2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find i\n",
    "new_list = []\n",
    "for i in sentences:\n",
    "    if((\" i \" in i) == True):\n",
    "        i = ' '.join([word for word in i.split() if word not in (stopwords.words('english'))])\n",
    "        i = i.split()\n",
    "        print(i)\n",
    "        break\n",
    "    else:\n",
    "        g = 0\n",
    "    \n",
    "try:        \n",
    "    for word in i:\n",
    "        if(len(word) == 1):\n",
    "            new_list.append(word)\n",
    "    print(new_list)\n",
    "\n",
    "\n",
    "    phrase = ' '.join(new_list)\n",
    "    print(phrase)\n",
    "    i = re.findall(r'i (\\d+)', phrase)\n",
    "    if((i == []) is True):\n",
    "        i = re.findall(r'(\\d+) i', phrase)\n",
    "    \n",
    "    print(i[0])\n",
    "    var3 = i[0]\n",
    "    print('i = ', var3)\n",
    "\n",
    "except IndexError as e:\n",
    "    var3 = 0\n",
    "    print(var3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find t\n",
    "new_list = []\n",
    "for i in sentences:\n",
    "    if((\" t \" in i) == True):\n",
    "        i = ' '.join([word for word in i.split() if word not in (stopwords.words('english'))])\n",
    "        i = i.split()\n",
    "        print(i)\n",
    "        break\n",
    "    else:\n",
    "        g = 0\n",
    "    \n",
    "try:        \n",
    "    for word in i:\n",
    "        if(len(word) == 1):\n",
    "            new_list.append(word)\n",
    "    print(new_list)\n",
    "\n",
    "\n",
    "    phrase = ' '.join(new_list)\n",
    "    print(phrase)\n",
    "    t = re.findall(r't (\\d+)', phrase)\n",
    "    if((t == []) is True):\n",
    "        t = re.findall(r'(\\d+) t', phrase)\n",
    "    \n",
    "    print(t[0])\n",
    "    var4 = t[0]\n",
    "    print('t = ', var4)\n",
    "\n",
    "except IndexError as e:\n",
    "    var4 = 0\n",
    "    print(var4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find cv\n",
    "new_list = []\n",
    "for i in sentences:\n",
    "    if((\" cv \" in i) == True):\n",
    "        i = ' '.join([word for word in i.split() if word not in (stopwords.words('english'))])\n",
    "        i = i.split()\n",
    "        print(i)\n",
    "        break\n",
    "    else:\n",
    "        g = 0\n",
    "        \n",
    "try:\n",
    "    for word in i:\n",
    "        if(len(word) <= 2):\n",
    "            new_list.append(word)\n",
    "    print(new_list)\n",
    "\n",
    "\n",
    "    phrase = ' '.join(new_list)\n",
    "    print(phrase)\n",
    "    cv = re.findall(r'cv (\\d+)', phrase)\n",
    "    print(cv[0])\n",
    "    var5 = cv[0]\n",
    "    print('cv = ', var5)\n",
    "\n",
    "except IndexError as e:\n",
    "    var5 = 0\n",
    "    print(var5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find v\n",
    "new_list = []\n",
    "for i in sentences:\n",
    "    if((\" v \" in i) == True):\n",
    "        i = ' '.join([word for word in i.split() if word not in (stopwords.words('english'))])\n",
    "        i = i.split()\n",
    "        print(i)\n",
    "        break\n",
    "    else:\n",
    "        g = 0\n",
    "    \n",
    "try:        \n",
    "    for word in i:\n",
    "        if(len(word) < 2):\n",
    "            new_list.append(word)\n",
    "    print(new_list)\n",
    "\n",
    "\n",
    "    phrase = ' '.join(new_list)\n",
    "    print(phrase)\n",
    "    v = re.findall(r'v (\\d+)', phrase)\n",
    "    if((v == []) is True):\n",
    "        v = re.findall(r'(\\d+) v', phrase)\n",
    "    \n",
    "    print(v[0])\n",
    "    var6 = v[0]\n",
    "    print('v = ', var6)\n",
    "\n",
    "except IndexError as e:\n",
    "    var6 = 0\n",
    "    print(var6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find ah\n",
    "new_list = []\n",
    "for i in sentences:\n",
    "    if((\" ah \" in i) == True):\n",
    "        i = ' '.join([word for word in i.split() if word not in (stopwords.words('english'))])\n",
    "        i = i.split()\n",
    "        print(i)\n",
    "        break\n",
    "    else:\n",
    "        g = 0\n",
    "    \n",
    "try:        \n",
    "    for word in i:\n",
    "        if(len(word) <= 2):\n",
    "            new_list.append(word)\n",
    "    print(new_list)\n",
    "\n",
    "\n",
    "    phrase = ' '.join(new_list)\n",
    "    print(phrase)\n",
    "    ah = re.findall(r'ah (\\d+)', phrase)\n",
    "    if((ah == []) is True):\n",
    "        ah = re.findall(r'(\\d+) ah', phrase)\n",
    "    \n",
    "    print(ah[0])\n",
    "    var7 = ah[0]\n",
    "    print('ah = ', var7)\n",
    "\n",
    "except IndexError as e:\n",
    "    var7 = 0\n",
    "    print(var7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find cg\n",
    "new_list = []\n",
    "for i in sentences:\n",
    "    if((\" cg \" in i) == True):\n",
    "        i = ' '.join([word for word in i.split() if word not in (stopwords.words('english'))])\n",
    "        i = i.split()\n",
    "        print(i)\n",
    "        break\n",
    "    else:\n",
    "        g = 0\n",
    "    \n",
    "try:        \n",
    "    for word in i:\n",
    "        if(len(word) <= 2):\n",
    "            new_list.append(word)\n",
    "    print(new_list)\n",
    "\n",
    "\n",
    "    phrase = ' '.join(new_list)\n",
    "    print(phrase)\n",
    "    cg = re.findall(r'cg (\\d+)', phrase)\n",
    "    if((cg == []) is True):\n",
    "        cg = re.findall(r'(\\d+) cg', phrase)\n",
    "    \n",
    "    print(cg[0])\n",
    "    var8 = cg[0]\n",
    "    print('cg = ', var8)\n",
    "\n",
    "except IndexError as e:\n",
    "    var8 = 0\n",
    "    print(var8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find g\n",
    "new_list = []\n",
    "for i in sentences:\n",
    "    if((\" ci \" in i) == True):\n",
    "        i = ' '.join([word for word in i.split() if word not in (stopwords.words('english'))])\n",
    "        i = i.split()\n",
    "        print(i)\n",
    "        break\n",
    "    else:\n",
    "        g = 0\n",
    "    \n",
    "try:        \n",
    "    for word in i:\n",
    "        if(len(word) <= 2):\n",
    "            new_list.append(word)\n",
    "    print(new_list)\n",
    "\n",
    "\n",
    "    phrase = ' '.join(new_list)\n",
    "    print(phrase)\n",
    "    ci = re.findall(r'ci (\\d+)', phrase)\n",
    "    if((ci == []) is True):\n",
    "        ci = re.findall(r'(\\d+) ci', phrase)\n",
    "    \n",
    "    print(ci[0])\n",
    "    var9 = ci[0]\n",
    "    print('ci = ', var9)\n",
    "\n",
    "except IndexError as e:\n",
    "    var9 = 0\n",
    "    print(var9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find ct\n",
    "new_list = []\n",
    "for i in sentences:\n",
    "    if((\" ct \" in i) == True):\n",
    "        i = ' '.join([word for word in i.split() if word not in (stopwords.words('english'))])\n",
    "        i = i.split()\n",
    "        print(i)\n",
    "        break\n",
    "    else:\n",
    "        g = 0\n",
    "    \n",
    "try:        \n",
    "    for word in i:\n",
    "        if(len(word) <= 2):\n",
    "            new_list.append(word)\n",
    "    print(new_list)\n",
    "\n",
    "\n",
    "    phrase = ' '.join(new_list)\n",
    "    print(phrase)\n",
    "    ct = re.findall(r'ct (\\d+)', phrase)\n",
    "    if((ct == []) is True):\n",
    "        ct = re.findall(r'(\\d+) ct', phrase)\n",
    "    \n",
    "    print(ct[0])\n",
    "    var10 = ct[0]\n",
    "    print('ct = ', var10)\n",
    "\n",
    "except IndexError as e:\n",
    "    var10 = 0\n",
    "    print(var10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find g\n",
    "new_list = []\n",
    "for i in sentences:\n",
    "    if((\" ti \" in i) == True):\n",
    "        i = ' '.join([word for word in i.split() if word not in (stopwords.words('english'))])\n",
    "        i = i.split()\n",
    "        print(i)\n",
    "        break\n",
    "    else:\n",
    "        g = 0\n",
    "    \n",
    "try:        \n",
    "    for word in i:\n",
    "        if(len(word) <= 2):\n",
    "            new_list.append(word)\n",
    "    print(new_list)\n",
    "\n",
    "\n",
    "    phrase = ' '.join(new_list)\n",
    "    print(phrase)\n",
    "    ti = re.findall(r'ti (\\d+)', phrase)\n",
    "    if((ti == []) is True):\n",
    "        ti = re.findall(r'(\\d+) ti', phrase)\n",
    "    \n",
    "    print(ti[0])\n",
    "    var11 = ti[0]\n",
    "    print('ti = ', var11)\n",
    "\n",
    "except IndexError as e:\n",
    "    var11 = 0\n",
    "    print(var11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find g\n",
    "new_list = []\n",
    "for i in sentences:\n",
    "    if((\" mm \" in i) == True):\n",
    "        i = ' '.join([word for word in i.split() if word not in (stopwords.words('english'))])\n",
    "        i = i.split()\n",
    "        print(i)\n",
    "        break\n",
    "    else:\n",
    "        g = 0\n",
    "    \n",
    "try:        \n",
    "    for word in i:\n",
    "        if(len(word) <= 2):\n",
    "            new_list.append(word)\n",
    "    print(new_list)\n",
    "\n",
    "\n",
    "    phrase = ' '.join(new_list)\n",
    "    print(phrase)\n",
    "    mm = re.findall(r'mm (\\d+)', phrase)\n",
    "    if((mm == []) is True):\n",
    "        mm = re.findall(r'(\\d+) mm', phrase)\n",
    "    \n",
    "    print(mm[0])\n",
    "    var12 = mm[0]\n",
    "    print('mm = ', var12)\n",
    "\n",
    "except IndexError as e:\n",
    "    var12 = 0\n",
    "    print(var12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find g\n",
    "new_list = []\n",
    "for i in sentences:\n",
    "    if((\" ptc \" in i) == True):\n",
    "        i = ' '.join([word for word in i.split() if word not in (stopwords.words('english'))])\n",
    "        i = i.split()\n",
    "        print(i)\n",
    "        break\n",
    "    else:\n",
    "        g = 0\n",
    "    \n",
    "try:        \n",
    "    for word in i:\n",
    "        if(len(word) <= 3):\n",
    "            new_list.append(word)\n",
    "    print(new_list)\n",
    "\n",
    "\n",
    "    phrase = ' '.join(new_list)\n",
    "    print(phrase)\n",
    "    ptc = re.findall(r'ptc (\\d+)', phrase)\n",
    "    if((ptc == []) is True):\n",
    "        ptc = re.findall(r'(\\d+) ptc', phrase)\n",
    "    \n",
    "    print(ptc[0])\n",
    "    var13 = ptc[0]\n",
    "    print('ptc = ', var13)\n",
    "\n",
    "except IndexError as e:\n",
    "    var13 = 0\n",
    "    print(var13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add row to Dataframe\n",
    "\n",
    "df = df.append({'glomerulis':var0, 'g':var2, 'i': var3, \n",
    "                't': var4, 'v': var5, 'ah': var6, 'cg': var7, \n",
    "                'ci': var8, 'ct': var9, 'ti': var10, 'cv': var11, \n",
    "                'mm': var12, 'ptc': var13, 'glom_scler': var1}, \n",
    "               ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correction des fautes d'orthographes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = fuzz.ratio(\"tubulointerstitial fibrosis\", \"tublointstitial fibris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(proba >= 90):\n",
    "    print('Same word')\n",
    "else:\n",
    "    print('Different word')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def get_cosine_sim(*strs): \n",
    "    vectors = [t for t in get_vectors(*strs)]\n",
    "    return cosine_similarity(vectors)\n",
    "    \n",
    "def get_vectors(*strs):\n",
    "    text = [t for t in strs]\n",
    "    vectorizer = CountVectorizer(text)\n",
    "    vectorizer.fit(text)\n",
    "    return vectorizer.transform(text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = list_text[rand_int_text()]\n",
    "text_1 = ' '.join(text_1)\n",
    "text_1 = ' '.join([word for word in text_1.split() if word not in (stopwords.words('english'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = list_text[rand_int_text()]\n",
    "text_2 = ' '.join(text_2)\n",
    "text_2 = ' '.join([word for word in text_2.split() if word not in (stopwords.words('english'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_text = get_vectors(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using NLTK library, we can do lot of text preprocesing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "#function to split text into word\n",
    "tokens = word_tokenize(pdfs[0]['content'])\n",
    "nltk.download('stopwords')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [w for w in tokens if not w in stop_words]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK provides several stemmer interfaces like Porter stemmer, #Lancaster Stemmer, Snowball Stemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "stems = []\n",
    "for t in tokens:    \n",
    "    stems.append(porter.stem(t))\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "reviews = df.review.str.cat(sep=' ')\n",
    "#function to split text into word\n",
    "tokens = word_tokenize(reviews)\n",
    "vocabulary = set(tokens)\n",
    "print(len(vocabulary))\n",
    "frequency_dist = nltk.FreqDist(tokens)\n",
    "sorted(frequency_dist,key=frequency_dist.__getitem__, reverse=True)[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "wordcloud = WordCloud().generate_from_frequencies(frequency_dist)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data file names:\n",
    "path = r'/home/robin/Documents/INSERM/files_to_test'\n",
    "filenames = glob.glob(path + \"/*.pdf\")\n",
    "\n",
    "pdfs = []\n",
    "for filename in filenames:\n",
    "    pdfs.append(parser.from_file(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banff_true = pd.read_csv('/home/robin/Documents/INSERM/valeurs.csv', sep=',', delimiter=None, header='infer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banff_true = banff_true.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vrai = pd.DataFrame()\n",
    "for i, line in enumerate(banff_true):\n",
    "    df_vrai = banff_true.loc[:, line]\n",
    "    df_vrai = pd.DataFrame(df_vrai)\n",
    "    print(df_vrai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vrai = pd.DataFrame(df_vrai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_vra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banff_true = pd.read_csv('/home/robin/Documents/INSERM/csv_trueValue_mayo/newcsv.csv', sep=',', delimiter=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banff_true.index = banff_true['banff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del banff_true['banff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banff = banff_true.copy()\n",
    "banff.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = np.array(['0', '1', '2', '3', '4'])\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(banff['true'], banff['mined'])\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True, title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = banff['true']==banff['mined']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpt=0\n",
    "for i in df:\n",
    "    if(i==True):\n",
    "        cpt+=1\n",
    "cpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_cosine_sim(*strs): \n",
    "    vectors = [t for t in get_vectors(*strs)]\n",
    "    return cosine_similarity(vectors)\n",
    "    \n",
    "def get_vectors(*strs):\n",
    "    text = [t for t in strs]\n",
    "    vectorizer = CountVectorizer(text)\n",
    "    vectorizer.fit(text)\n",
    "    return vectorizer.transform(text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cosine_sim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_vectors(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_text[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test scrap variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_pdf[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text detection\n",
    "\n",
    "start = 'LIGHT MICROSCOPY'\n",
    "end = 'IMMUNOFLUORESCEN'\n",
    "\n",
    "def loop_text(start, end, liste_pdf):\n",
    "    list_text = []\n",
    "    for idx, text in enumerate(liste_pdf):\n",
    "        text = (text.split(start))[1].split(end)[0].lower()\n",
    "        text = text.lower().strip().replace('\\n', ' ').replace('  ', ' ').replace('[^\\w\\s]','')\n",
    "        for i in medical_flat_list:\n",
    "            if i in text:\n",
    "                text = text.replace(i, get_banff_code(medical_words, i))\n",
    "\n",
    "        for i in degrees_flat_list:\n",
    "            if i in text:\n",
    "                text = text.replace(i, get_degree(degrees, i))\n",
    "        list_text.append(split_into_sentences(text))\n",
    "    return list_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop_text(start, end, liste_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
